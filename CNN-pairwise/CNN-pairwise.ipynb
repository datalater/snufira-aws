{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vcoabuarly 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/books_text_full/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13538\n",
      "[('Tick', 2045), ('He', 1384), ('The', 1296), ('said', 1071), ('like', 1003), ('Paul', 881), ('back', 777), ('Sofia', 730), ('one', 687), ('George', 670), ('Master', 573), ('could', 562), ('Jane', 527), ('But', 512), ('Sato', 495), ('looked', 483), ('know', 480), ('It', 475), ('Chu', 461), ('time', 455), ('didnt', 449), ('And', 445), ('eyes', 441), ('right', 429), ('She', 426), ('something', 423), ('hed', 403), ('man', 393), ('felt', 380), ('What', 376), ('around', 374), ('asked', 364), ('see', 360), ('Im', 353), ('away', 346), ('Mothball', 339), ('face', 334), ('get', 331), ('air', 327), ('Rutger', 324), ('would', 312), ('thought', 309), ('made', 308), ('us', 307), ('head', 307), ('You', 302), ('way', 296), ('thing', 277), ('things', 276), ('think', 273)]\n",
      "13538\n",
      "\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]로 저장했습니다.\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]을 [vocab]으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # white space 기준으로 tokenize 합니다.\n",
    "    tokens = doc.split()\n",
    "    # 각 token에서 모든 구두점을 삭제합니다.\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # 각 token에서 alaphabet으로만 이루어지지 않은 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # 각 token에서 stopwrods를 삭제합니다.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # 각 token에서 1글자 이하인 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 텍스트 파일을 불러와서 vocab에 추가하는 함수\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # 텍스트 파일을 불러옵니다.\n",
    "    doc = load_doc(filename)\n",
    "    # 텍스트 파일을 clean toekn으로 리턴합니다.\n",
    "    tokens = clean_doc(doc)\n",
    "    # clean token을 vocab에 추가합니다.\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    for filename in listdir(directory):\n",
    "        # 인덱스가 새겨진 파일 이름과 is_train 인자를 기준으로 test set으로 분류할 모든 파일을 건너뜁니다.\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 텍스트 파일을 불러와서 vocab에 추가하는 함수를 실행합니다.\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    # 각 문장을 하나의 텍스트 일부로 바꿉니다.\n",
    "    data = '\\n'.join(lines)\n",
    "    # 파일을 쓰기 모드로 엽니다.\n",
    "    file = open(filename, 'w')\n",
    "    # 변환한 텍스트를 파일에 씁니다.\n",
    "    file.write(data)\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "\n",
    "# vocab을 Counter() 객체로 할당합니다.\n",
    "vocab = Counter()\n",
    "# 폴더를 지정하고 폴더 내 모든 문서를 vocab에 추가합니다.\n",
    "process_docs(data_path, vocab, True)\n",
    "# vocab의 크기를 출력합니다.\n",
    "print(len(vocab))\n",
    "# vocab에서 가장 많이 등장한 50개 단어를 출력합니다.\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# token을 min_occurence 기준으로 유지합니다.\n",
    "min_occurence = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))\n",
    "# token을 vocab 파일로 저장합니다.\n",
    "save_list(tokens, 'corpusToLines_vocab.txt')\n",
    "print(\"\\n# 단어 {}개의 [corpusToLines_vocab.txt]로 저장했습니다.\".format(len(tokens)))\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'corpusToLines_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"# 단어 {}개의 [{}]을 [vocab]으로 불러왔습니다.\".format(len(vocab), vocab_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 코퍼스를 문장 단위 리스트로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 19160\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 문서를 cleaned lines(=tokens)으로 리턴합니다.\n",
    "# tokens = line, token = word\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = []\n",
    "    lines = doc.split(\".\")\n",
    "    for line in lines:\n",
    "        # 공백 기준으로 token을 나눕니다.\n",
    "        tokens = line.split()\n",
    "        # 구두점 제거\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # vocab에 있는 단어만 추출합니다.\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    for filename in listdir(directory):\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 파일을 불러옵니다.\n",
    "        doc = load_doc(path)\n",
    "        # 파일을 개끗하게 만들어서 token으로 리턴합니다.\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # 리스트 객체에 추가합니다.\n",
    "        lines += doc_lines\n",
    "    return lines\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'fantasy_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# 모든 training set을 불러옵니다.\n",
    "train_docs = process_docs(data_path, vocab, True)\n",
    "sentences = train_docs\n",
    "print('Total training sentences: %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 15240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12674\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 문서를 cleaned lines(=tokens)으로 리턴합니다.\n",
    "# tokens = line, token = word\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = []\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # 공백 기준으로 token을 나눕니다.\n",
    "        tokens = line.split()\n",
    "        # 구두점 제거\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # vocab에 있는 단어만 추출합니다.\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    i = 0\n",
    "    for filename in listdir(directory):\n",
    "        # 인덱스가 새겨진 파일 이름과 is_train 인자를 기준으로 test set으로 분류할 모든 파일을 건너뜁니다.\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 파일을 불러옵니다.\n",
    "        doc = load_doc(path)\n",
    "        # 파일을 개끗하게 만들어서 token으로 리턴합니다.\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # 리스트 객체에 추가합니다.\n",
    "        lines += doc_lines\n",
    "        i += 1\n",
    "        if i % 300 == 0:\n",
    "            print(lines)\n",
    "    return lines\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'fantasy_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# 모든 training set을 불러옵니다.\n",
    "train_docs = process_docs(data_path, vocab, True)\n",
    "sentences = train_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    "\n",
    "# word2vec 모델을 훈련시킵니다.\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "# 모델의 vocabulary size를 요약합니다.\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" % len(words))\n",
    "\n",
    "# 모델을 ASCII 포맷으로 저장합니다.\n",
    "filename = 'fantasy_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', 'soulikens'],\n",
       " [],\n",
       " ['Master',\n",
       "  'George',\n",
       "  'sat',\n",
       "  'study',\n",
       "  'lights',\n",
       "  'dimmed',\n",
       "  'Muffintops',\n",
       "  'purring',\n",
       "  'corner',\n",
       "  'first',\n",
       "  'light',\n",
       "  'dawns',\n",
       "  'birth',\n",
       "  'still',\n",
       "  'hour',\n",
       "  'He',\n",
       "  'stared',\n",
       "  'wall',\n",
       "  'fascinating',\n",
       "  'thing',\n",
       "  'Realities',\n",
       "  'stapled',\n",
       "  'see',\n",
       "  'whenever',\n",
       "  'wished',\n",
       "  'knot',\n",
       "  'wood',\n",
       "  'paneling',\n",
       "  'knot',\n",
       "  'two',\n",
       "  'eyes',\n",
       "  'mouth',\n",
       "  'looked',\n",
       "  'right',\n",
       "  'reason',\n",
       "  'reminded',\n",
       "  'boy',\n",
       "  'named',\n",
       "  'Atticus',\n",
       "  'Higginbottom']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)\n",
    "sentences[4:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word2vec 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# 텍스트 파일을 clean token으로 리턴하는 함수\n",
    "def clean_doc(doc, vocab):\n",
    "    # white space 기준으로 tokenize 합니다.\n",
    "    tokens = doc.split()\n",
    "    # 각 token에서 모든 구두점을 삭제합니다.\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # 각 token에서 보카에 없는 단어는 걸러냅니다.\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    for filename in listdir(directory):\n",
    "        # 인덱스가 새겨진 파일 이름과 is_train 인자를 기준으로 test set으로 분류할 모든 파일을 건너뜁니다.\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 파일을 불러옵니다.\n",
    "        doc = load_doc(path)\n",
    "        # 파일을 개끗하게 만들어서 token으로 리턴합니다.\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # 리스트 객체에 추가합니다.\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# embedding을 dictionary로 불러옵니다.\n",
    "def load_embedding(filename):\n",
    "    # embedding을 메모리에 올려두되 첫번째 줄은 생략합니다.\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # 단어와 벡터를 연결하는 map을 생성합니다.\n",
    "    embedding = {}\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # 딕셔너리의 key는 단어에 대한 문자열이고, value는 벡터에 대한 numpy array입니다.\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# 불러온 embedding을 기준으로 Embedding layer의 weight matrix를 생성합니다.\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # 전체 vocabulary 크기에 unknown words에 대한 숫자 0을 추가합니다.\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # (vocab_size, 100) 만큼 weight matrix의 모든 값을 0으로 초기화해서 생성합니다.\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # Tokenizer의 정수 맵핑을 이옹하여 vocab의 모든 단어에 해당하는 벡터를 저장합니다.\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'fantasy_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# 모든 training set을 불러옵니다.\n",
    "train_docs = process_docs(data_path, vocab, True)\n",
    "# 텍스트 문서를 정수로 맵핑시키는 tokenizer를 정의합니다.\n",
    "tokenizer = Tokenizer()\n",
    "# tokenizer를 모든 training 문서에 적용합니다.\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "# 텍스트 문서를 정수의 나열로 인코드합니다.\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# 가장 길이가 긴 문서의 길이를 구하고, 그에 맞춰 패딩합니다.\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:180036\n"
     ]
    }
   ],
   "source": [
    "# vocab_size를 정의합니다.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab_size:{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180036"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271436\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성하기 (filter_size = [3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 271436, 100)       18003600  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 271434, 100)       30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 135717, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 135714, 100)       40100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 67857, 100)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 67853, 100)        50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 33926, 100)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3392600)           0         \n",
      "=================================================================\n",
      "Total params: 18,123,900\n",
      "Trainable params: 120,300\n",
      "Non-trainable params: 18,003,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers import Merge\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "# 파일에서 embedding을 불러옵니다.\n",
    "raw_embedding = load_embedding('fantasy_embedding_word2vec.txt')\n",
    "# 벡터를 올바른 순서로 정렬합니다.\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# embedding layer를 만듭니다.\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "with K.tf.device('/gpu:0'):\n",
    "    filter_sizes = [3,4,5]\n",
    "    drop_out_rate = 0.5\n",
    "    hidden_dims = 50\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    for idx, sz in enumerate(filter_sizes):\n",
    "        model.add(Conv1D(filters=100, kernel_size=sz, activation='relu', strides=1, padding='valid'))\n",
    "        model.add((MaxPooling1D(pool_size=2)))\n",
    "        \n",
    "    \n",
    "#     model.add(Dropout(drop_out_rate))  \n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(hidden_dims, activation=\"relu\"))\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Xtrain으로 3개씩 pair 만들기\n",
    "# pair = [[Xtrain[i], Xtrain[i+1], Xtrain[i+2]] for i in range(len(Xtrain)) if i < len(Xtrain)-2]\n",
    "# assert len(Xtrain) - 2 == len(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "with open('pair.pkl', 'wb') as f:\n",
    "    cPickle.dump(pair, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(pair[0], pair[1], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 모델 정의\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(Conv1D(filters=200, kernel_size=5, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# print(model.summary)\n",
    "\n",
    "# # network를 컴파일합니다.\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # network를 training data에 fit 합니다.\n",
    "# model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# # 평가\n",
    "# loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pair 나누기\n",
    "\n",
    "+ 최종 모델의 출력을 리스트로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN input data 묶기 (0, 1, 2), (1, 2, 3), (2, 3, 4)\n",
    "def pairing(input2):\n",
    "    newinput=[]\n",
    "    for i in range(len(input2)-2):\n",
    "        holdlist=[]\n",
    "        newinput.append(holdlist)\n",
    "        holdlist.append(input2[i])\n",
    "        holdlist.append(input2[i+1])\n",
    "        holdlist.append(input2[i+2])\n",
    "    return newinput\n",
    "            \n",
    "\n",
    "# cosine similarity\n",
    "import math\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "\n",
    "# (0, 2):1, (1, 3):2, (2, 4):3\n",
    "# 이 중에 왼쪽 pair cosine similarity\n",
    "def side_cosine_similarity(v1_set, v2_set):\n",
    "    first=cosine_similarity(v1_set[0], v2_set[0])\n",
    "    second=cosine_similarity(v1_set[2], v2_set[2])\n",
    "    \n",
    "    return (first+second)/2\n",
    "\n",
    "\n",
    "def center_cosine_similarity(v1_set, v2_set):\n",
    "    return cosine_similarity(v1_set[1], v2_set[1])\n",
    "\n",
    "\n",
    "def loss_sum(data_array):\n",
    "    pairs = pairing(data_array)\n",
    "    sum = 0\n",
    "    for n1 in range(len(pairs)):\n",
    "        for n2 in range(n1, len(pairs)):\n",
    "            if not n1 == n2:\n",
    "                loss=side_cosine_similarity(pairs[n1], pairs[n2])-center_cosine_similarity(pairs[n1], pairs[n2])\n",
    "                sum+=loss**2\n",
    "    return sum\n",
    "\n",
    "model_output\n",
    "loss_sum(model_output)\n",
    "\n",
    "for i in range(len(pairs)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
