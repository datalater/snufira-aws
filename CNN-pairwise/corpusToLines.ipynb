{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../data/books_text_full/test/')\n",
    "data_path = '../data/books_text_full/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define vocabualry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13538\n",
      "[('Tick', 2045), ('He', 1384), ('The', 1296), ('said', 1071), ('like', 1003), ('Paul', 881), ('back', 777), ('Sofia', 730), ('one', 687), ('George', 670), ('Master', 573), ('could', 562), ('Jane', 527), ('But', 512), ('Sato', 495), ('looked', 483), ('know', 480), ('It', 475), ('Chu', 461), ('time', 455), ('didnt', 449), ('And', 445), ('eyes', 441), ('right', 429), ('She', 426), ('something', 423), ('hed', 403), ('man', 393), ('felt', 380), ('What', 376), ('around', 374), ('asked', 364), ('see', 360), ('Im', 353), ('away', 346), ('Mothball', 339), ('face', 334), ('get', 331), ('air', 327), ('Rutger', 324), ('would', 312), ('thought', 309), ('made', 308), ('head', 307), ('us', 307), ('You', 302), ('way', 296), ('thing', 277), ('things', 276), ('They', 273)]\n",
      "13538\n",
      "Loading completed: corpusToLines_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # white space 기준으로 tokenize 합니다.\n",
    "    tokens = doc.split()\n",
    "    # 각 token에서 모든 구두점을 삭제합니다.\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # 각 token에서 alaphabet으로만 이루어지지 않은 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # 각 token에서 stopwrods를 삭제합니다.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # 각 token에서 1글자 이하인 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 텍스트 파일을 불러와서 vocab에 추가하는 함수\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # 텍스트 파일을 불러옵니다.\n",
    "    doc = load_doc(filename)\n",
    "    # 텍스트 파일을 clean toekn으로 리턴합니다.\n",
    "    tokens = clean_doc(doc)\n",
    "    # clean token을 vocab에 추가합니다.\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    for filename in listdir(directory):\n",
    "        # 인덱스가 새겨진 파일 이름과 is_train 인자를 기준으로 test set으로 분류할 모든 파일을 건너뜁니다.\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 텍스트 파일을 불러와서 vocab에 추가하는 함수를 실행합니다.\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "    \n",
    "def save_list(lines, filename):\n",
    "    # 각 문장을 하나의 텍스트 일부로 바꿉니다.\n",
    "    data = '\\n'.join(lines)\n",
    "    # 파일을 쓰기 모드로 엽니다.\n",
    "    file = open(filename, 'w')\n",
    "    # 변환한 텍스트를 파일에 씁니다.\n",
    "    file.write(data)\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    \n",
    "# vocab을 Counter() 객체로 할당합니다.\n",
    "vocab = Counter()\n",
    "# 폴더를 지정하고 폴더 내 모든 문서를 vocab에 추가합니다.\n",
    "process_docs(data_path, vocab, True)\n",
    "# vocab의 크기를 출력합니다.\n",
    "print(len(vocab))\n",
    "# vocab에서 가장 많이 등장한 50개 단어를 출력합니다.\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# token을 min_occurence 기준으로 유지합니다.\n",
    "min_occurence = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))\n",
    "# token을 vocab 파일로 저장합니다.\n",
    "save_list(tokens, 'corpusToLines_vocab.txt')\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'corpusToLines_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"\\nLoading completed: {}\".format(vocab_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpusToLines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 파일 문장 개수: 1738\n",
      "전체 파일 문장 개수: 3476\n"
     ]
    }
   ],
   "source": [
    "# 파일을 str 객체로 불러오기\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# 파일에서 clean token과 clean lines 추출하기\n",
    "total_lines = []\n",
    "lines = [i for i in text.splitlines() if i]  # 공백 문장 제거\n",
    "\n",
    "total_lines += lines\n",
    "lines2 = list(lines)\n",
    "total_lines += lines2\n",
    "\n",
    "print(\"\\n현재 파일 문장 개수:\", len(lines))\n",
    "print(\"전체 파일 문장 개수:\", len(total_lines))\n",
    "# total_lines.append(lines)\n",
    "# len(total_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 7620\n",
      "문장 7620개가 [total_lines.txt]로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def doc_to_lines(doc):\n",
    "    total_lines = []\n",
    "    lines = [i for i in doc.splitlines() if i]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    total_lines = []\n",
    "    for filename in listdir(directory):\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_lines(doc)\n",
    "        \n",
    "        total_lines += doc_lines\n",
    "\n",
    "    return total_lines\n",
    "\n",
    "\n",
    "def save_total_lines(lines, filename):\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# 모든 training set을 불러옵니다.\n",
    "train_docs = process_docs(data_path, vocab, True)\n",
    "sentences = train_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    "\n",
    "save_list(sentences, 'total_lines.txt')\n",
    "print(\"문장 {}개가 [total_lines.txt]로 저장되었습니다.\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## clean_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"total_lines.txt\"\n",
    "\n",
    "# 파일을 str 객체로 불러오기\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "clean_lines = [i for i in text.splitlines() if len(i) > 3 if \".\" in i] # 3개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함\n",
    "save_list(clean_lines, 'clean_lines.txt')\n",
    "print(\"문장 {}개가 [clean_lines.txt]로 저장되었습니다.\".format(len(clean_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 6929개가 [clean_lines.txt]로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "def doc_to_clean_lines(filename):\n",
    "    total_lines = load_doc(filename)\n",
    "    clean_lines = [i for i in text.splitlines() if len(i) > 3 if \".\" in i] # 3개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함\n",
    "    \n",
    "    return clean_lines\n",
    "\n",
    "filename = \"total_lines.txt\"\n",
    "clean_lines = doc_to_clean_lines(filename)\n",
    "save_list(clean_lines, 'clean_lines.txt')\n",
    "print(\"문장 {}개가 [clean_lines.txt]로 저장되었습니다.\".format(len(clean_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vocab에 있는 단어로만 구성된 문장**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"clean_lines.txt\"\n",
    "\n",
    "# 파일을 str 객체로 불러오기\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It']\n"
     ]
    }
   ],
   "source": [
    "for i in text.splitlines():\n",
    "    words = i.split()\n",
    "    words = [word for word in words if word in vocab]\n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"was\" in vocab:\n",
    "    print(\"INCLUDED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"clean_lines.txt\"\n",
    "\n",
    "# 파일을 str 객체로 불러오기\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "for i in text.splitlines():\n",
    "    \n",
    "        \n",
    "\n",
    "clean_lines = [i for i in text.splitlines() if len(i) > 3 if \".\" in i] # 3개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함\n",
    "save_list(clean_lines, 'clean_lines.txt')\n",
    "print(\"문장 {}개가 [clean_lines.txt]로 저장되었습니다.\".format(len(clean_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def doc_to_clean_lines(doc):\n",
    "    total_lines = []\n",
    "    lines = [i for i in doc.splitlines() if len(i) > 3 if \".\" in i]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    total_lines = []\n",
    "    for filename in listdir(directory):\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_lines(doc)\n",
    "        \n",
    "        total_lines += doc_lines\n",
    "\n",
    "    return total_lines\n",
    "\n",
    "\n",
    "def save_total_lines(lines, filename):\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
