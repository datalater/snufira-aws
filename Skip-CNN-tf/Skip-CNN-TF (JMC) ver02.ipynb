{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/books_text_full/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13538\n",
      "\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]로 저장했습니다.\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]을 [vocab]으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "vocab = Counter()\n",
    "process_docs(data_path, vocab, True)\n",
    "# print(len(vocab))\n",
    "# print(vocab.most_common(50))\n",
    "\n",
    "# token을 min_occurence 기준으로 유지합니다.\n",
    "min_occurence = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))\n",
    "# token을 vocab 파일로 저장합니다.\n",
    "save_list(tokens, 'corpusToLines_vocab.txt')\n",
    "print(\"\\n# 단어 {}개의 [corpusToLines_vocab.txt]로 저장했습니다.\".format(len(tokens)))\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'corpusToLines_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"# 단어 {}개의 [{}]을 [vocab]으로 불러왔습니다.\".format(len(vocab), vocab_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. corpusToLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13th_Reality-4.txt : 2388\n",
      "13th_Reality-2.txt : 3494\n",
      "13th_Reality-1.txt : 1738\n",
      "\n",
      "# 문장 7620개의 [total_lines.txt]로 저장했습니다.\n",
      "# unique words in [total_lines.txt]: [54]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def doc_to_lines(doc):\n",
    "    total_lines = []\n",
    "    lines = [i.lower() for i in doc.splitlines() if i]  # 공백 문장 제거 및 모든 문장 소문자 변경\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def process_directory(data_path):\n",
    "    total_lines = []\n",
    "    for filename in listdir(data_path):\n",
    "        filepath = data_path + '/' + filename\n",
    "        doc = load_doc(filepath)\n",
    "        lines = doc_to_lines(doc)\n",
    "        print(filename, \":\", len(lines))\n",
    "        total_lines += lines\n",
    "    return total_lines\n",
    "\n",
    "\n",
    "sentences = process_directory(data_path)\n",
    "save_list(sentences, 'total_lines.txt')\n",
    "print(\"\\n# 문장 {}개의 [total_lines.txt]로 저장했습니다.\".format(len(sentences)))\n",
    "filename = 'total_lines.txt'\n",
    "total_lines = load_doc(filename)\n",
    "total_lines = [i for i in total_lines.splitlines()]\n",
    "total_vocab = set()\n",
    "for i in total_lines:\n",
    "    total_vocab.update(i)\n",
    "print(\"# unique words in [total_lines.txt]: [{}]\".format(len(total_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 문장 6921개가 [clean_lines.txt]로 저장되었습니다.\n",
      "# unique words in [clean_lines.txt]: [51]\n"
     ]
    }
   ],
   "source": [
    "def doc_to_clean_lines(filename):\n",
    "    total_lines = load_doc(filename)\n",
    "    clean_lines = [i.lower() for i in total_lines.splitlines() if len(i) > 5 if \".\" in i] # 5개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함\n",
    "    \n",
    "    return clean_lines\n",
    "\n",
    "filename = \"total_lines.txt\"\n",
    "clean_lines = doc_to_clean_lines(filename)\n",
    "save_list(clean_lines, 'clean_lines.txt')\n",
    "print(\"# 문장 {}개가 [clean_lines.txt]로 저장되었습니다.\".format(len(clean_lines)))\n",
    "filename = 'clean_lines.txt'\n",
    "clean_lines = load_doc(filename)\n",
    "clean_lines = [i for i in clean_lines.splitlines()]\n",
    "clean_vocab = set()\n",
    "for i in clean_lines:\n",
    "    clean_vocab.update(i)\n",
    "print(\"# unique words in [clean_lines.txt]: [{}]\".format(len(clean_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 문장 4832개가 [vocab_lines.txt]로 저장되었습니다.\n",
      "# unique words in [vocab_lines.txt]: [28]\n"
     ]
    }
   ],
   "source": [
    "def doc_to_vocab_lines(filename):\n",
    "    clean_lines = load_doc(filename)\n",
    "    vocab_lines = []\n",
    "    for i in clean_lines.splitlines():\n",
    "        words = i.split()\n",
    "        words = [word for word in words if word in vocab]\n",
    "        words = [word for word in words if len(words) >= 5]\n",
    "        vocab_line = \" \".join(words)\n",
    "        if len(vocab_line):\n",
    "            vocab_line += \".\"\n",
    "            vocab_line = [vocab_line]\n",
    "            vocab_lines += vocab_line\n",
    "    \n",
    "    return vocab_lines\n",
    "\n",
    "filename = \"clean_lines.txt\"\n",
    "vocab_lines = doc_to_vocab_lines(filename)\n",
    "save_list(vocab_lines, 'vocab_lines.txt')\n",
    "print(\"# 문장 {}개가 [vocab_lines.txt]로 저장되었습니다.\".format(len(vocab_lines)))\n",
    "filename = 'vocab_lines.txt'\n",
    "vocab_lines = load_doc(filename)\n",
    "vocab_lines = [i for i in vocab_lines.splitlines()]\n",
    "vocab_vocab = set()\n",
    "for i in vocab_lines:\n",
    "    vocab_vocab.update(i)\n",
    "print(\"# unique words in [vocab_lines.txt]: [{}]\".format(len(vocab_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['master', 'sat', 'lights', 'purring', 'first', 'light', 'birth', 'still', 'hour', 'stared', 'wall', 'fascinating', 'thing', 'realities', 'stapled', 'see', 'whenever', 'knot', 'wood', 'knot', 'two', 'eyes', 'mouth', 'looked', 'reason', 'reminded', 'boy', 'named.']\n"
     ]
    }
   ],
   "source": [
    "filename = \"vocab_lines.txt\"\n",
    "\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "vocab_lines = [i for i in text.splitlines()]\n",
    "\n",
    "list_lines = []\n",
    "for i in vocab_lines:\n",
    "    i = i.split()\n",
    "    list_lines.append(i)\n",
    "\n",
    "print(list_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences:4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10065\n",
      "Wordvector size: 100\n",
      "Embedding size: 10065x100\n",
      "\n",
      "# word2vec 파일 [fantasy_embedding_word2vec.txt]이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "sentences = list_lines\n",
    "print(\"Total training sentences:{}\".format(len(sentences)))\n",
    "\n",
    "wv_sz = 100\n",
    "# word2vec 모델을 훈련시킵니다.\n",
    "model = Word2Vec(sentences, size=wv_sz, window=5, workers=8, min_count=1)\n",
    "# 모델의 vocabulary size를 요약합니다.\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" % len(words))\n",
    "print(\"Wordvector size: %d\" % (wv_sz))\n",
    "print(\"Embedding size: {}x{}\".format(len(words), wv_sz))\n",
    "\n",
    "# 모델을 ASCII 포맷으로 저장합니다.\n",
    "filename = 'fantasy_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)\n",
    "print(\"\\n# word2vec 파일 [{}]이 저장되었습니다.\".format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Use pre-trained word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "# [vocab_lines]가 [encoded_lines]로 인코딩 및 패딩 되었습니다. (max_length:62)\n",
      "--------------------------------------------------------------------------------\n",
      "BEFORE: \n",
      "master sat lights purring first light birth still hour stared wall fascinating thing realities stapled see whenever knot wood knot two eyes mouth looked reason reminded boy named.\n",
      "\n",
      "AFTER: \n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 18 20 21 22 23 24\n",
      " 25 26 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "# 최종 [x_data] (max_length: 62)\n",
      "--------------------------------------------------------------------------------\n",
      "EXAMPLE: \n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 18 20 21 22 23 24\n",
      " 25 26 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "# 최종 [vocab_dict] (vocab_size: 8329)\n",
      "--------------------------------------------------------------------------------\n",
      "EXAMPLE: \n",
      "[master] is mapped to [1].\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "max_length = max([len(s.split()) for s in vocab_lines])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "encoded_lines = np.array(list(vocab_processor.fit_transform(vocab_lines)))\n",
    "print(\"-\"*80,\"# [vocab_lines]가 [encoded_lines]로 인코딩 및 패딩 되었습니다. (max_length:{})\".format(max_length), \"-\"*80, sep='\\n')\n",
    "print(\"BEFORE: \\n{}\".format(vocab_lines[0]))\n",
    "print(\"\\nAFTER: \\n{}\".format(encoded_lines[0]))\n",
    "\n",
    "x_data = np.array(list(encoded_lines))\n",
    "print(\"\\n\", \"-\"*80,\"# 최종 [x_data] (max_length: {})\".format(max_length), \"-\"*80, sep='\\n')\n",
    "print(\"EXAMPLE: \\n{}\".format(x_data[0]))\n",
    "\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_size = len(vocab_dict.keys())\n",
    "print(\"\\n\", \"-\"*80,\"# 최종 [vocab_dict] (vocab_size: {})\".format(vocab_size), \"-\"*80, sep='\\n')\n",
    "print(\"EXAMPLE: \\n[{}] is mapped to [{}].\".format(vocab_lines[0].split()[0], vocab_dict[vocab_lines[0].split()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../Skip-CNN-tf/fantasy_embedding_word2vec.txt!\n",
      "10065\n"
     ]
    }
   ],
   "source": [
    "def load_word2vec(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    for line in lines:\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded {}!'.format(filename))\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "filename = '../Skip-CNN-tf/fantasy_embedding_word2vec.txt'\n",
    "vocab,embd = load_word2vec(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. tensorflow로 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        return self.h_pool_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() should return None, not 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-67aac3899a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             num_filters=128)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() should return None, not 'Tensor'"
     ]
    }
   ],
   "source": [
    "cnn = TextCNN(\n",
    "            sequence_length=x_data.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=100,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
