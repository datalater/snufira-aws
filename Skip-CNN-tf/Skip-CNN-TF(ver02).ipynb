{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/books_text_full/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Define vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13538\n",
      "[('Tick', 2045), ('He', 1384), ('The', 1296), ('said', 1071), ('like', 1003), ('Paul', 881), ('back', 777), ('Sofia', 730), ('one', 687), ('George', 670), ('Master', 573), ('could', 562), ('Jane', 527), ('But', 512), ('Sato', 495), ('looked', 483), ('know', 480), ('It', 475), ('Chu', 461), ('time', 455), ('didnt', 449), ('And', 445), ('eyes', 441), ('right', 429), ('She', 426), ('something', 423), ('hed', 403), ('man', 393), ('felt', 380), ('What', 376), ('around', 374), ('asked', 364), ('see', 360), ('Im', 353), ('away', 346), ('Mothball', 339), ('face', 334), ('get', 331), ('air', 327), ('Rutger', 324), ('would', 312), ('thought', 309), ('made', 308), ('head', 307), ('us', 307), ('You', 302), ('way', 296), ('thing', 277), ('things', 276), ('go', 273)]\n",
      "13538\n",
      "\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]로 저장했습니다.\n",
      "# 단어 13538개의 [corpusToLines_vocab.txt]을 [vocab]으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 텍스트 파일의 내용을 변수 text로 리턴하는 함수\n",
    "def load_doc(filename):\n",
    "    # read only로 파일을 엽니다.\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    # 모든 텍스트를 읽습니다.\n",
    "    text = file.read()\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # white space 기준으로 tokenize 합니다.\n",
    "    tokens = doc.split()\n",
    "    # 각 token에서 모든 구두점을 삭제합니다.\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # 각 token에서 alaphabet으로만 이루어지지 않은 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # 각 token에서 stopwrods를 삭제합니다.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # 각 token에서 1글자 이하인 모든 단어를 삭제합니다.\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 텍스트 파일을 불러와서 vocab에 추가하는 함수\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # 텍스트 파일을 불러옵니다.\n",
    "    doc = load_doc(filename)\n",
    "    # 텍스트 파일을 clean toekn으로 리턴합니다.\n",
    "    tokens = clean_doc(doc)\n",
    "    # clean token을 vocab에 추가합니다.\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# 폴더에 있는 모든 문서를 vocab에 추가하는 함수\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    # 폴더에 있는 모든 파일을 순회합니다.\n",
    "    for filename in listdir(directory):\n",
    "        # 인덱스가 새겨진 파일 이름과 is_train 인자를 기준으로 test set으로 분류할 모든 파일을 건너뜁니다.\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # 폴더에 있는 파일의 절대 경로를 구합니다.\n",
    "        path = directory + '/' + filename\n",
    "        # 텍스트 파일을 불러와서 vocab에 추가하는 함수를 실행합니다.\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    # 각 문장을 하나의 텍스트 일부로 바꿉니다.\n",
    "    data = '\\n'.join(lines)\n",
    "    # 파일을 쓰기 모드로 엽니다.\n",
    "    file = open(filename, 'w')\n",
    "    # 변환한 텍스트를 파일에 씁니다.\n",
    "    file.write(data)\n",
    "    # 파일을 닫습니다.\n",
    "    file.close()\n",
    "\n",
    "# vocab을 Counter() 객체로 할당합니다.\n",
    "vocab = Counter()\n",
    "# 폴더를 지정하고 폴더 내 모든 문서를 vocab에 추가합니다.\n",
    "process_docs(data_path, vocab, True)\n",
    "# vocab의 크기를 출력합니다.\n",
    "print(len(vocab))\n",
    "# vocab에서 가장 많이 등장한 50개 단어를 출력합니다.\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# token을 min_occurence 기준으로 유지합니다.\n",
    "min_occurence = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))\n",
    "# token을 vocab 파일로 저장합니다.\n",
    "save_list(tokens, 'corpusToLines_vocab.txt')\n",
    "print(\"\\n# 단어 {}개의 [corpusToLines_vocab.txt]로 저장했습니다.\".format(len(tokens)))\n",
    "\n",
    "# 보카를 불러옵니다.\n",
    "vocab_filename = 'corpusToLines_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"# 단어 {}개의 [{}]을 [vocab]으로 불러왔습니다.\".format(len(vocab), vocab_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. corpusToLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13th_Reality-4.txt : 2388\n",
      "13th_Reality-2.txt : 3494\n",
      "13th_Reality-1.txt : 1738\n",
      "\n",
      "# 문장 7620개의 [total_lines.txt]로 저장했습니다.\n",
      "# unique words in [total_lines.txt]: [54]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r', errors='replace')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def doc_to_lines(doc):\n",
    "    total_lines = []\n",
    "    lines = [i.lower() for i in doc.splitlines() if i]  # 공백 문장 제거 및 모든 문장 소문자 변경\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def process_directory(data_path):\n",
    "    total_lines = []\n",
    "    for filename in listdir(data_path):\n",
    "        filepath = data_path + '/' + filename\n",
    "        doc = load_doc(filepath)\n",
    "        lines = doc_to_lines(doc)\n",
    "        print(filename, \":\", len(lines))\n",
    "        total_lines += lines\n",
    "    return total_lines\n",
    "\n",
    "\n",
    "sentences = process_directory(data_path)\n",
    "save_list(sentences, 'total_lines.txt')\n",
    "print(\"\\n# 문장 {}개의 [total_lines.txt]로 저장했습니다.\".format(len(sentences)))\n",
    "filename = 'total_lines.txt'\n",
    "total_lines = load_doc(filename)\n",
    "total_lines = [i for i in total_lines.splitlines()]\n",
    "total_vocab = set()\n",
    "for i in total_lines:\n",
    "    total_vocab.update(i)\n",
    "print(\"# unique words in [total_lines.txt]: [{}]\".format(len(total_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 문장 6921개가 [clean_lines.txt]로 저장되었습니다.\n",
      "# unique words in [clean_lines.txt]: [51]\n"
     ]
    }
   ],
   "source": [
    "def doc_to_clean_lines(filename):\n",
    "    total_lines = load_doc(filename)\n",
    "    clean_lines = [i.lower() for i in total_lines.splitlines() if len(i) > 5 if \".\" in i] # 5개 단어 이상으로 이루어지고 마침표가 있는 문장만 포함\n",
    "    \n",
    "    return clean_lines\n",
    "\n",
    "filename = \"total_lines.txt\"\n",
    "clean_lines = doc_to_clean_lines(filename)\n",
    "save_list(clean_lines, 'clean_lines.txt')\n",
    "print(\"# 문장 {}개가 [clean_lines.txt]로 저장되었습니다.\".format(len(clean_lines)))\n",
    "filename = 'clean_lines.txt'\n",
    "clean_lines = load_doc(filename)\n",
    "clean_lines = [i for i in clean_lines.splitlines()]\n",
    "clean_vocab = set()\n",
    "for i in clean_lines:\n",
    "    clean_vocab.update(i)\n",
    "print(\"# unique words in [clean_lines.txt]: [{}]\".format(len(clean_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 문장 4832개가 [vocab_lines.txt]로 저장되었습니다.\n",
      "# unique words in [vocab_lines.txt]: [28]\n"
     ]
    }
   ],
   "source": [
    "def doc_to_vocab_lines(filename):\n",
    "    clean_lines = load_doc(filename)\n",
    "    vocab_lines = []\n",
    "    for i in clean_lines.splitlines():\n",
    "        words = i.split()\n",
    "        words = [word for word in words if word in vocab]\n",
    "        words = [word for word in words if len(words) >= 5]\n",
    "        vocab_line = \" \".join(words)\n",
    "        if len(vocab_line):\n",
    "            vocab_line += \".\"\n",
    "            vocab_line = [vocab_line]\n",
    "            vocab_lines += vocab_line\n",
    "    \n",
    "    return vocab_lines\n",
    "\n",
    "filename = \"clean_lines.txt\"\n",
    "vocab_lines = doc_to_vocab_lines(filename)\n",
    "save_list(vocab_lines, 'vocab_lines.txt')\n",
    "print(\"# 문장 {}개가 [vocab_lines.txt]로 저장되었습니다.\".format(len(vocab_lines)))\n",
    "filename = 'vocab_lines.txt'\n",
    "vocab_lines = load_doc(filename)\n",
    "vocab_lines = [i for i in vocab_lines.splitlines()]\n",
    "vocab_vocab = set()\n",
    "for i in vocab_lines:\n",
    "    vocab_vocab.update(i)\n",
    "print(\"# unique words in [vocab_lines.txt]: [{}]\".format(len(vocab_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['master', 'sat', 'lights', 'purring', 'first', 'light', 'birth', 'still', 'hour', 'stared', 'wall', 'fascinating', 'thing', 'realities', 'stapled', 'see', 'whenever', 'knot', 'wood', 'knot', 'two', 'eyes', 'mouth', 'looked', 'reason', 'reminded', 'boy', 'named.']\n"
     ]
    }
   ],
   "source": [
    "filename = \"vocab_lines.txt\"\n",
    "\n",
    "file = open(filename, 'r', errors='replace')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "vocab_lines = [i for i in text.splitlines()]\n",
    "\n",
    "list_lines = []\n",
    "for i in vocab_lines:\n",
    "    i = i.split()\n",
    "    list_lines.append(i)\n",
    "\n",
    "print(list_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences:4832\n",
      "Vocabulary size: 10065\n",
      "Wordvector size: 100\n",
      "Embedding size: 10065x100\n",
      "\n",
      "# word2vec 파일 [fantasy_embedding_word2vec.txt]이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "sentences = list_lines\n",
    "print(\"Total training sentences:{}\".format(len(sentences)))\n",
    "\n",
    "wv_sz = 100\n",
    "# word2vec 모델을 훈련시킵니다.\n",
    "model = Word2Vec(sentences, size=wv_sz, window=5, workers=8, min_count=1)\n",
    "# 모델의 vocabulary size를 요약합니다.\n",
    "words = list(model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" % len(words))\n",
    "print(\"Wordvector size: %d\" % (wv_sz))\n",
    "print(\"Embedding size: {}x{}\".format(len(words), wv_sz))\n",
    "\n",
    "# 모델을 ASCII 포맷으로 저장합니다.\n",
    "filename = 'fantasy_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)\n",
    "print(\"\\n# word2vec 파일 [{}]이 저장되었습니다.\".format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Use pre-trained word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "# [vocab_lines]가 [encoded_lines]로 인코딩 및 패딩 되었습니다. (max_length:62)\n",
      "--------------------------------------------------------------------------------\n",
      "BEFORE: \n",
      "master sat lights purring first light birth still hour stared wall fascinating thing realities stapled see whenever knot wood knot two eyes mouth looked reason reminded boy named.\n",
      "\n",
      "AFTER: \n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 18 20 21 22 23 24\n",
      " 25 26 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "# 최종 [x_data] (max_length: 62)\n",
      "--------------------------------------------------------------------------------\n",
      "EXAMPLE: \n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 18 20 21 22 23 24\n",
      " 25 26 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "# 최종 [vocab_dict] (vocab_size: 8329)\n",
      "--------------------------------------------------------------------------------\n",
      "EXAMPLE: \n",
      "[master] is mapped to [1].\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "max_length = max([len(s.split()) for s in vocab_lines])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "encoded_lines = np.array(list(vocab_processor.fit_transform(vocab_lines)))\n",
    "print(\"-\"*80,\"# [vocab_lines]가 [encoded_lines]로 인코딩 및 패딩 되었습니다. (max_length:{})\".format(max_length), \"-\"*80, sep='\\n')\n",
    "print(\"BEFORE: \\n{}\".format(vocab_lines[0]))\n",
    "print(\"\\nAFTER: \\n{}\".format(encoded_lines[0]))\n",
    "\n",
    "x_data = np.array(list(encoded_lines))\n",
    "print(\"\\n\", \"-\"*80,\"# 최종 [x_data] (max_length: {})\".format(max_length), \"-\"*80, sep='\\n')\n",
    "print(\"EXAMPLE: \\n{}\".format(x_data[0]))\n",
    "\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "vocab_size = len(vocab_dict.keys())\n",
    "print(\"\\n\", \"-\"*80,\"# 최종 [vocab_dict] (vocab_size: {})\".format(vocab_size), \"-\"*80, sep='\\n')\n",
    "print(\"EXAMPLE: \\n[{}] is mapped to [{}].\".format(vocab_lines[0].split()[0], vocab_dict[vocab_lines[0].split()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./fantasy_embedding_word2vec.txt!\n",
      "10065\n"
     ]
    }
   ],
   "source": [
    "def load_word2vec(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    for line in lines:\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded {}!'.format(filename))\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "filename = './fantasy_embedding_word2vec.txt'\n",
    "vocab,embd = load_word2vec(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd, dtype=np.float32)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.509359  , -0.967897  ,  0.64973199,  0.38006401,  0.063952  ,\n",
       "       -0.123214  ,  0.70174801,  1.12439501,  0.045738  ,  0.71135497,\n",
       "       -0.054567  ,  0.35919201, -1.56762004, -1.03178704, -0.410905  ,\n",
       "        0.44969299, -1.13110399,  0.33462399, -0.40008599,  2.7377851 ,\n",
       "       -0.55689102,  0.543998  ,  0.46350601, -1.71855295, -0.116501  ,\n",
       "        0.37546301,  0.71789598, -0.248209  , -0.38674   ,  0.67623597,\n",
       "       -0.478746  , -0.65623897,  0.206597  , -0.073647  , -0.71558899,\n",
       "       -0.20276999, -0.823892  ,  0.093382  , -0.214516  ,  0.81545198,\n",
       "        0.27880001, -0.83634698, -0.011659  , -0.264779  ,  0.22314601,\n",
       "        0.475694  , -0.48697501, -0.78541899,  0.21345   ,  1.12521803,\n",
       "       -0.52066302,  1.55839396,  0.50588298, -0.48045999,  0.29283899,\n",
       "       -0.54532999, -1.12173998, -0.25499699,  0.520495  ,  0.59847701,\n",
       "       -1.34818196, -0.53955001, -1.58476603,  0.049398  ,  0.943097  ,\n",
       "       -0.40384001, -0.70289999,  1.02950299,  0.130946  , -0.49247   ,\n",
       "       -1.01579404, -0.117993  , -0.301137  , -0.749744  , -0.721771  ,\n",
       "       -0.68507999,  0.98294699,  0.76173699,  0.27841401, -0.97669601,\n",
       "        0.25807199,  0.381327  , -0.166122  , -0.31777799, -0.13445   ,\n",
       "        0.327317  ,  1.20028102, -0.79844701,  0.069729  , -0.52376097,\n",
       "        0.13124201,  0.76155198,  0.63970101,  0.47589701,  0.33181399,\n",
       "        0.621315  ,  0.41081399,  0.422252  ,  0.14392599, -0.385782  ], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. tensorflow로 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters**\n",
    "\n",
    "+ `wv_sz` = 100 (III. word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**layer input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(dtype=tf.int32, shape=[None, sequence_length], name=\"input_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**embedding layer and embedding lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    \n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x) \n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**layer output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoutput = embedded_chars_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_1/ExpandDims:0' shape=(?, 62, 100, 1) dtype=float32>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chars_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Convolution and Max-pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = wv_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "def inference(input_x):\n",
    "    \"\"\"\n",
    "    Build the Skip-CNN model.\n",
    "    \n",
    "    Args:    \n",
    "        embedding\n",
    "    Returns:    \n",
    "        cnn_output\n",
    "    \"\"\"\n",
    "    \n",
    "    # embedding layer\n",
    "    \n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "        W = tf.Variable(\n",
    "            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "            name=\"W\")\n",
    "        embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    \n",
    "    # convolution layer\n",
    "    \n",
    "    filter_sizes = [3, 4, 5]\n",
    "    num_filters = 128\n",
    "\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, wv_sz, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv2d(\n",
    "                embedded_chars_expanded,\n",
    "                W,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            # Max-pooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = tf.concat( pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "    return h_pool_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_sentence = batch_size\n",
    "\n",
    "def cossim(a, b):\n",
    "    dot=tf.cast(tf.tensordot(a, b, axes=1), tf.float32)\n",
    "\n",
    "    norm1=tf.sqrt(tf.cast(tf.tensordot(a, a, axes=1), tf.float32))\n",
    "    norm2=tf.sqrt(tf.cast(tf.tensordot(b, b, axes=1), tf.float32))\n",
    "\n",
    "    mycossi=tf.div(dot, tf.multiply(norm1, norm2))\n",
    "    \n",
    "    return mycossi\n",
    "    \n",
    "def loss(cnn_output):\n",
    "    \"\"\"Side cosine_similarity - Center cosine_similarity\n",
    "  \n",
    "  Args:\n",
    "    logits: cnn_output ( sentence_number * 384 : 3 kernel * 128 kernel number )\n",
    "    \n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "    total_cossi = tf.zeros(1)\n",
    "   \n",
    "    for si in range(num_sentence - 3):\n",
    "        for ssi in range(si, num_sentence-2):\n",
    "            if not si == ssi:\n",
    "                \n",
    "                cossi1 = cossim(cnn_output[si], cnn_output[ssi])\n",
    "                cossi2 = cossim(cnn_output[si+1], cnn_output[ssi+2])\n",
    "                cossi3 = cossim(cnn_output[si+2], cnn_output[ssi+2])\n",
    "                \n",
    "                cossi= tf.subtract(tf.div(tf.add(cossi1, cossi3),2), cossi2)\n",
    "                \n",
    "                total_cossi = tf.add(total_cossi,cossi)\n",
    "                \n",
    "    return total_cossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_5:0' shape=(?, 384) dtype=float32>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = inference(input_x)\n",
    "loss = loss(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(?, 384), dtype=float32)\n",
      "Tensor(\"Add_57:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# loss까지 tensor결과 \n",
    "\n",
    "print(num_filters_total)\n",
    "print(h_pool)\n",
    "print(h_pool_flat)\n",
    "print(logits)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4832, 62)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "[ 0.]\n",
      "Epoch: 0001\n",
      "Avg. cost: [ 0.]\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 32\n",
    "total_batch = int(len(x_data) / batch_size)\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(x_data), batch_size):    \n",
    "        x_batch = x_data[i:i+batch_size]\n",
    "\n",
    "        _, loss_val = sess.run([optimizer, loss],\n",
    "                                feed_dict={input_x: x_batch})\n",
    "\n",
    "        print(loss_val)\n",
    "        total_loss += loss_val\n",
    "    \n",
    "    print(\"Epoch: %04d\" % (epoch + 1))\n",
    "    print(\"Avg. cost: {}\".format(total_loss / total_batch))\n",
    "\n",
    "print(\"최적화 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 18, 20, 21, 22, 23, 24, 25, 26, 27,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.cast(x_data[0], tf.float32)\n",
    "b = tf.cast(x_data[1], tf.float32)\n",
    "c = tf.cast(x_data[2], tf.float32)\n",
    "\n",
    "l = tf.cast(x_data[1], tf.float32)\n",
    "m = tf.cast(x_data[2], tf.float32)\n",
    "n = tf.cast(x_data[3], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sub_435:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cossi1 = cossim(a, l)\n",
    "cossi2 = cossim(b, m)\n",
    "cossi3 = cossim(c, n)\n",
    "\n",
    "cossi= tf.subtract(tf.div(tf.add(cossi1, cossi3),2), cossi2)\n",
    "print(cossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06829083"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(x_data[0], dtype=np.float32)\n",
    "b = np.array(x_data[1], dtype=np.float32)\n",
    "c = np.array(x_data[2], dtype=np.float32)\n",
    "\n",
    "l = np.array(x_data[1], dtype=np.float32)\n",
    "m = np.array(x_data[2], dtype=np.float32)\n",
    "n = np.array(x_data[3], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cossi1 = cossim(a, l)\n",
    "cossi2 = cossim(b, m)\n",
    "cossi3 = cossim(c, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossi = (cossi1 + cossi3)/2 - cossi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06829083"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(cossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value conv-maxpool-5_6/b\n\t [[Node: conv-maxpool-5_6/b/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv-maxpool-5_6/b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv-maxpool-5_6/b)]]\n\nCaused by op 'conv-maxpool-5_6/b/read', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-127-230afc4dddee>\", line 1, in <module>\n    logits = inference(input_x)\n  File \"<ipython-input-117-029133c2ca4a>\", line 33, in inference\n    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 344, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1490, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv-maxpool-5_6/b\n\t [[Node: conv-maxpool-5_6/b/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv-maxpool-5_6/b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv-maxpool-5_6/b)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv-maxpool-5_6/b\n\t [[Node: conv-maxpool-5_6/b/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv-maxpool-5_6/b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv-maxpool-5_6/b)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-bbb763721eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv-maxpool-5_6/b\n\t [[Node: conv-maxpool-5_6/b/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv-maxpool-5_6/b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv-maxpool-5_6/b)]]\n\nCaused by op 'conv-maxpool-5_6/b/read', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-127-230afc4dddee>\", line 1, in <module>\n    logits = inference(input_x)\n  File \"<ipython-input-117-029133c2ca4a>\", line 33, in inference\n    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 344, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1490, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv-maxpool-5_6/b\n\t [[Node: conv-maxpool-5_6/b/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv-maxpool-5_6/b\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv-maxpool-5_6/b)]]\n"
     ]
    }
   ],
   "source": [
    "logits0, logits1 = sess.run([logits[0], logits[1]], feed_dict={input_x: x_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sess.run(cossim(logits0, logits1))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
